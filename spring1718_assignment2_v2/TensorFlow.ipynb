{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "# What's this TensorFlow business?\n",
    "\n",
    "You've written a lot of code in this assignment to provide a whole host of neural network functionality. Dropout, Batch Norm, and 2D convolutions are some of the workhorses of deep learning in computer vision. You've also worked hard to make your code efficient and vectorized.\n",
    "\n",
    "For the last part of this assignment, though, we're going to leave behind your beautiful codebase and instead migrate to one of two popular deep learning frameworks: in this instance, TensorFlow (or PyTorch, if you switch over to that notebook)\n",
    "\n",
    "#### What is it?\n",
    "TensorFlow is a system for executing computational graphs over Tensor objects, with native support for performing backpropogation for its Variables. In it, we work with Tensors which are n-dimensional arrays analogous to the numpy ndarray.\n",
    "\n",
    "#### Why?\n",
    "\n",
    "* Our code will now run on GPUs! Much faster training. Writing your own modules to run on GPUs is beyond the scope of this class, unfortunately.\n",
    "* We want you to be ready to use one of these frameworks for your project so you can experiment more efficiently than if you were writing every feature you want to use by hand. \n",
    "* We want you to stand on the shoulders of giants! TensorFlow and PyTorch are both excellent frameworks that will make your lives a lot easier, and now that you understand their guts, you are free to use them :) \n",
    "* We want you to be exposed to the sort of deep learning code you might run into in academia or industry. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "zh"
   },
   "source": [
    "# TensorFlow业务是什么？\n",
    "\n",
    "您已在此分配中编写了大量代码，以提供大量神经网络功能。 Dropout，Batch Norm和2D convolutions是计算机视觉中深度学习的一些主力。您还努力使代码高效且矢量化。\n",
    "\n",
    "但是，对于这个任务的最后一部分，我们将留下你漂亮的代码库，而是迁移到两个流行的深度学习框架之一：在这个例子中，TensorFlow（或PyTorch，如果你切换到那个笔记本）\n",
    "\n",
    "#### 它是什么？\n",
    "TensorFlow是一个用于在Tensor对象上执行计算图形的系统，具有对其变量执行反向传播的本机支持。在其中，我们使用Tensors，它们是n维数组，类似于numpy ndarray。\n",
    "\n",
    "#### 为什么？\n",
    "\n",
    "*我们的代码现在可以在GPU上运行了！更快的培训。不幸的是，编写自己的模块以在GPU上运行超出了本课程的范围。*我们希望您准备好为您的项目使用其中一个框架，这样您就可以比编写想要手动使用的每个功能更有效地进行实验。\n",
    "*我们希望你站在巨人的肩膀上！ TensorFlow和PyTorch都是优秀的框架，可以让你的生活更轻松，现在你了解他们的胆量，你可以自由使用它们:)*我们希望您接触到学术界或行业可能会遇到的深度学习代码。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How will I learn TensorFlow?\n",
    "\n",
    "TensorFlow has many excellent tutorials available, including those from [Google themselves](https://www.tensorflow.org/get_started/get_started).\n",
    "\n",
    "Otherwise, this notebook will walk you through much of what you need to do to train models in TensorFlow. See the end of the notebook for some links to helpful tutorials if you want to learn more or need further clarification on topics that aren't fully explained here.\n",
    "\n",
    "\n",
    "# Table of Contents\n",
    "\n",
    "This notebook has 5 parts. We will walk through TensorFlow at three different levels of abstraction, which should help you better understand it and prepare you for working on your project.\n",
    "\n",
    "1. Preparation: load the CIFAR-10 dataset.\n",
    "2. Barebone TensorFlow: we will work directly with low-level TensorFlow graphs. \n",
    "3. Keras Model API: we will use `tf.keras.Model` to define arbitrary neural network architecture. \n",
    "4. Keras Sequential API: we will use `tf.keras.Sequential` to define a linear feed-forward network very conveniently. \n",
    "5. CIFAR-10 open-ended challenge: please implement your own network to get as high accuracy as possible on CIFAR-10. You can experiment with any layer, optimizer, hyperparameters or other advanced features. \n",
    "\n",
    "Here is a table of comparison:\n",
    "\n",
    "| API           | Flexibility | Convenience |\n",
    "|---------------|-------------|-------------|\n",
    "| Barebone      | High        | Low         |\n",
    "| `tf.keras.Model`     | High        | Medium      |\n",
    "| `tf.keras.Sequential` | Low         | High        |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I: Preparation\n",
    "\n",
    "First, we load the CIFAR-10 dataset. This might take a few minutes to download the first time you run it, but after that the files should be cached on disk and loading should be faster.\n",
    "\n",
    "In previous parts of the assignment we used CS231N-specific code to download and read the CIFAR-10 dataset; however the `tf.keras.datasets` package in TensorFlow provides prebuilt utility functions for loading many common datasets.\n",
    "\n",
    "For the purposes of this assignment we will still write our own code to preprocess the data and iterate through it in minibatches. The `tf.data` package in TensorFlow provides tools for automating this process, but working with this package adds extra complication and is beyond the scope of this notebook. However using `tf.data` can be much more efficient than the simple approach used in this notebook, so you should consider using it for your project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import timeit\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 32, 32, 3)\n",
      "Train labels shape:  (49000,) int32\n",
      "Validation data shape:  (1000, 32, 32, 3)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (10000, 32, 32, 3)\n",
      "Test labels shape:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "def load_cifar10(num_training=49000, num_validation=1000, num_test=10000):\n",
    "    \"\"\"\n",
    "    Fetch the CIFAR-10 dataset from the web and perform preprocessing to prepare\n",
    "    it for the two-layer neural net classifier. These are the same steps as\n",
    "    we used for the SVM, but condensed to a single function.\n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 dataset and use appropriate data types and shapes\n",
    "    cifar10 = tf.keras.datasets.cifar10.load_data()\n",
    "    (X_train, y_train), (X_test, y_test) = cifar10\n",
    "    X_train = np.asarray(X_train, dtype=np.float32)\n",
    "    y_train = np.asarray(y_train, dtype=np.int32).flatten()\n",
    "    X_test = np.asarray(X_test, dtype=np.float32)\n",
    "    y_test = np.asarray(y_test, dtype=np.int32).flatten()\n",
    "\n",
    "    # Subsample the data\n",
    "    mask = range(num_training, num_training + num_validation)\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = range(num_training)\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = range(num_test)\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "\n",
    "    # Normalize the data: subtract the mean pixel and divide by std\n",
    "    mean_pixel = X_train.mean(axis=(0, 1, 2), keepdims=True)\n",
    "    std_pixel = X_train.std(axis=(0, 1, 2), keepdims=True)\n",
    "    X_train = (X_train - mean_pixel) / std_pixel\n",
    "    X_val = (X_val - mean_pixel) / std_pixel\n",
    "    X_test = (X_test - mean_pixel) / std_pixel\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "NHW = (0, 1, 2)\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_cifar10()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape, y_train.dtype)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation: Dataset object\n",
    "\n",
    "For our own convenience we'll define a lightweight `Dataset` class which lets us iterate over data and labels. This is not the most flexible or most efficient way to iterate through data, but it will serve our purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(object):\n",
    "    def __init__(self, X, y, batch_size, shuffle=False):\n",
    "        \"\"\"\n",
    "        Construct a Dataset object to iterate over data X and labels y\n",
    "        \n",
    "        Inputs:\n",
    "        - X: Numpy array of data, of any shape\n",
    "        - y: Numpy array of labels, of any shape but with y.shape[0] == X.shape[0]\n",
    "        - batch_size: Integer giving number of elements per minibatch\n",
    "        - shuffle: (optional) Boolean, whether to shuffle the data on each epoch\n",
    "        \"\"\"\n",
    "        assert X.shape[0] == y.shape[0], 'Got different numbers of data and labels'\n",
    "        self.X, self.y = X, y\n",
    "        self.batch_size, self.shuffle = batch_size, shuffle\n",
    "\n",
    "    def __iter__(self):\n",
    "        N, B = self.X.shape[0], self.batch_size\n",
    "        idxs = np.arange(N)\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(idxs)\n",
    "        return iter((self.X[i:i+B], self.y[i:i+B]) for i in range(0, N, B))\n",
    "\n",
    "\n",
    "train_dset = Dataset(X_train, y_train, batch_size=64, shuffle=True)\n",
    "val_dset = Dataset(X_val, y_val, batch_size=64, shuffle=False)\n",
    "test_dset = Dataset(X_test, y_test, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (64, 32, 32, 3) (64,)\n",
      "1 (64, 32, 32, 3) (64,)\n",
      "2 (64, 32, 32, 3) (64,)\n",
      "3 (64, 32, 32, 3) (64,)\n",
      "4 (64, 32, 32, 3) (64,)\n",
      "5 (64, 32, 32, 3) (64,)\n",
      "6 (64, 32, 32, 3) (64,)\n"
     ]
    }
   ],
   "source": [
    "# We can iterate through a dataset like this:\n",
    "for t, (x, y) in enumerate(train_dset):\n",
    "    print(t, x.shape, y.shape)\n",
    "    if t > 5: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can optionally **use GPU by setting the flag to True below**. It's not neccessary to use a GPU for this assignment; if you are working on Google Cloud then we recommend that you do not use a GPU, as it will be significantly more expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# Set up some global variables\n",
    "USE_GPU = True\n",
    "\n",
    "if USE_GPU:\n",
    "    device = '/device:GPU:0'\n",
    "else:\n",
    "    device = '/cpu:0'\n",
    "\n",
    "# Constant to control how often we print when training models\n",
    "print_every = 100\n",
    "\n",
    "print('Using device: ', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "# Part II: Barebone TensorFlow\n",
    "TensorFlow ships with various high-level APIs which make it very convenient to define and train neural networks; we will cover some of these constructs in Part III and Part IV of this notebook. In this section we will start by building a model with basic TensorFlow constructs to help you better understand what's going on under the hood of the higher-level APIs.\n",
    "\n",
    "TensorFlow is primarily a framework for working with **static computational graphs**. Nodes in the computational graph are Tensors which will hold n-dimensional arrays when the graph is run; edges in the graph represent functions that will operate on Tensors when the graph is run to actually perform useful computation.\n",
    "\n",
    "This means that a typical TensorFlow program is written in two distinct phases:\n",
    "\n",
    "1. Build a computational graph that describes the computation that you want to perform. This stage doesn't actually perform any computation; it just builds up a symbolic representation of your computation. This stage will typically define one or more `placeholder` objects that represent inputs to the computational graph.\n",
    "2. Run the computational graph many times. Each time the graph is run you will specify which parts of the graph you want to compute, and pass a `feed_dict` dictionary that will give concrete values to any `placeholder`s in the graph.\n",
    "\n",
    "### TensorFlow warmup: Flatten Function\n",
    "\n",
    "We can see this in action by defining a simple `flatten` function that will reshape image data for use in a fully-connected network.\n",
    "\n",
    "In TensorFlow, data for convolutional feature maps is typically stored in a Tensor of shape N x H x W x C where:\n",
    "\n",
    "- N is the number of datapoints (minibatch size)\n",
    "- H is the height of the feature map\n",
    "- W is the width of the feature map\n",
    "- C is the number of channels in the feature map\n",
    "\n",
    "This is the right way to represent the data when we are doing something like a 2D convolution, that needs spatial understanding of where the intermediate features are relative to each other. When we use fully connected affine layers to process the image, however, we want each datapoint to be represented by a single vector -- it's no longer useful to segregate the different channels, rows, and columns of the data. So, we use a \"flatten\" operation to collapse the `H x W x C` values per representation into a single long vector. The flatten function below first reads in the value of N from a given batch of data, and then returns a \"view\" of that data. \"View\" is analogous to numpy's \"reshape\" method: it reshapes x's dimensions to be N x ??, where ?? is allowed to be anything (in this case, it will be H x W x C, but we don't need to specify that explicitly). \n",
    "\n",
    "**NOTE**: TensorFlow and PyTorch differ on the default Tensor layout; TensorFlow uses N x H x W x C but PyTorch uses N x C x H x W."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "zh"
   },
   "source": [
    "# 第二部分：准系统TensorFlow\n",
    "TensorFlow附带各种高级API，可以非常方便地定义和训练神经网络;我们将在本笔记本的第三部分和第四部分中介绍其中一些结构。在本节中，我们将首先构建一个具有基本TensorFlow结构的模型，以帮助您更好地了解更高级API的内容。\n",
    "\n",
    "TensorFlow主要是用于处理**静态计算图**的框架。计算图中的节点是Tensors，当图形运行时，它将保持n维数组;图中的边表示在运行图以实际执行有用计算时将在张量上运行的函数。\n",
    "\n",
    "这意味着典型的TensorFlow程序分为两个不同的阶段：\n",
    "\n",
    "1.构建一个描述您要执行的计算的计算图。这个阶段实际上不执行任何计算;它只是建立了计算的符号表示。该阶段通常将定义一个或多个“占位符”对象，这些对象表示计算图的输入。\n",
    "2.多次运行计算图。每次运行图形时，您将指定要计算的图形的哪些部分，并传递一个“feed_dict”字典，该字典将为图形中的任何“占位符”提供具体值。\n",
    "\n",
    "### TensorFlow预热：展平功能\n",
    "\n",
    "我们可以通过定义一个简单的“flatten”函数来实现这一点，该函数将重塑图像数据以便在完全连接的网络中使用。\n",
    "\n",
    "在TensorFlow中，卷积特征图的数据通常存储在形状 N x H x W x C 的张量中，其中：\n",
    "\n",
    " -  N是数据点的数量（小批量大小）\n",
    " -  H是要素图的高度\n",
    " -  W是要素图的宽度\n",
    " -  C是要素图中的通道数\n",
    "\n",
    "当我们进行类似2D卷积的事情时，这是表示数据的正确方法，需要空间理解中间特征相对于彼此的位置。然而，当我们使用完全连接的仿射层来处理图像时，我们希望每个数据点由单个向量表示 - 分离数据的不同通道，行和列不再有用。因此，我们使用“展平”操作将每个表示的“H x W x C”值折叠成单个长向量。下面的展平函数首先从给定的一批数据中读取N的值，然后返回该数据的“视图”。 “视图”类似于numpy的“重塑”方法：它将x的尺寸重塑为N x ??，其中??允许是任何东西（在这种情况下，它将是H x W x C，但我们不需要明确指定）。\n",
    "\n",
    "**注意**：TensorFlow和PyTorch在默认的Tensor布局上有所不同; TensorFlow使用N x H x W x C但PyTorch使用N x C x H x W."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(x):\n",
    "    \"\"\"    \n",
    "    Input:\n",
    "    - TensorFlow Tensor of shape (N, D1, ..., DM)\n",
    "    \n",
    "    Output:\n",
    "    - TensorFlow Tensor of shape (N, D1 * ... * DM)\n",
    "    \"\"\"\n",
    "    N = tf.shape(x)[0]\n",
    "    return tf.reshape(x, (N, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:  <class 'tensorflow.python.framework.ops.Tensor'> Tensor(\"Placeholder:0\", dtype=float32, device=/device:GPU:0)\n",
      "x_flat:  <class 'tensorflow.python.framework.ops.Tensor'> Tensor(\"Reshape:0\", shape=(?, ?), dtype=float32, device=/device:GPU:0)\n",
      "\n",
      "x_np:\n",
      " [[[ 0  1  2  3]\n",
      "  [ 4  5  6  7]\n",
      "  [ 8  9 10 11]]\n",
      "\n",
      " [[12 13 14 15]\n",
      "  [16 17 18 19]\n",
      "  [20 21 22 23]]] \n",
      "\n",
      "x_flat_np:\n",
      " [[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11.]\n",
      " [12. 13. 14. 15. 16. 17. 18. 19. 20. 21. 22. 23.]] \n",
      "\n",
      "x_np:\n",
      " [[[ 0  1]\n",
      "  [ 2  3]\n",
      "  [ 4  5]]\n",
      "\n",
      " [[ 6  7]\n",
      "  [ 8  9]\n",
      "  [10 11]]] \n",
      "\n",
      "x_flat_np:\n",
      " [[ 0.  1.  2.  3.  4.  5.]\n",
      " [ 6.  7.  8.  9. 10. 11.]]\n"
     ]
    }
   ],
   "source": [
    "def test_flatten():\n",
    "    # Clear the current TensorFlow graph.\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    # Stage I: Define the TensorFlow graph describing our computation.\n",
    "    # In this case the computation is trivial: we just want to flatten\n",
    "    # a Tensor using the flatten function defined above.\n",
    "    \n",
    "    # Our computation will have a single input, x. We don't know its\n",
    "    # value yet, so we define a placeholder which will hold the value\n",
    "    # when the graph is run. We then pass this placeholder Tensor to\n",
    "    # the flatten function; this gives us a new Tensor which will hold\n",
    "    # a flattened view of x when the graph is run. The tf.device\n",
    "    # context manager tells TensorFlow whether to place these Tensors\n",
    "    # on CPU or GPU.\n",
    "    # 这里确定了使用的设备和占位符类型\n",
    "    with tf.device(device):\n",
    "        x = tf.placeholder(tf.float32)\n",
    "        x_flat = flatten(x)\n",
    "    \n",
    "    # At this point we have just built the graph describing our computation,\n",
    "    # but we haven't actually computed anything yet. If we print x and x_flat\n",
    "    # we see that they don't hold any data; they are just TensorFlow Tensors\n",
    "    # representing values that will be computed when the graph is run.\n",
    "    # 这里并没有开始计算，只是用tensor表示了数据\n",
    "    print('x: ', type(x), x)\n",
    "    print('x_flat: ', type(x_flat), x_flat)\n",
    "    print()\n",
    "    \n",
    "    # We need to use a TensorFlow Session object to actually run the graph.\n",
    "    with tf.Session() as sess:\n",
    "        # Construct concrete values of the input data x using numpy\n",
    "        # 构造具体数据，不过这里的数据格式有什么要求？\n",
    "        x_np = np.arange(24).reshape((2, 3, 4))\n",
    "        print('x_np:\\n', x_np, '\\n')\n",
    "    \n",
    "        # Run our computational graph to compute a concrete output value.\n",
    "        # The first argument to sess.run tells TensorFlow which Tensor\n",
    "        # we want it to compute the value of; the feed_dict specifies\n",
    "        # values to plug into all placeholder nodes in the graph. The\n",
    "        # resulting value of x_flat is returned from sess.run as a\n",
    "        # numpy array.\n",
    "        # 运行计算图开始计算\n",
    "        # 第一个参数决定了要计算的tansor，而feed_dict指定了要插入占位符节点的数据\n",
    "        # 整体返回一个 numpy数组\n",
    "        x_flat_np = sess.run(x_flat, feed_dict={x: x_np})\n",
    "        print('x_flat_np:\\n', x_flat_np, '\\n')\n",
    "\n",
    "        # We can reuse the same graph to perform the same computation\n",
    "        # with different input data\n",
    "        # 可以使用同一个图来计算不同的数据\n",
    "        x_np = np.arange(12).reshape((2, 3, 2))\n",
    "        print('x_np:\\n', x_np, '\\n')\n",
    "        x_flat_np = sess.run(x_flat, feed_dict={x: x_np})\n",
    "        print('x_flat_np:\\n', x_flat_np)\n",
    "test_flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "### Barebones TensorFlow: Two-Layer Network\n",
    "We will now implement our first neural network with TensorFlow: a fully-connected ReLU network with two hidden layers and no biases on the CIFAR10 dataset. For now we will use only low-level TensorFlow operators to define the network; later we will see how to use the higher-level abstractions provided by `tf.keras` to simplify the process.\n",
    "\n",
    "We will define the forward pass of the network in the function `two_layer_fc`; this will accept TensorFlow Tensors for the inputs and weights of the network, and return a TensorFlow Tensor for the scores. It's important to keep in mind that calling the `two_layer_fc` function **does not** perform any computation; instead it just sets up the computational graph for the forward computation. To actually run the network we need to enter a TensorFlow Session and feed data to the computational graph.\n",
    "\n",
    "After defining the network architecture in the `two_layer_fc` function, we will test the implementation by setting up and running a computational graph, feeding zeros to the network and checking the shape of the output.\n",
    "\n",
    "It's important that you read and understand this implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "zh"
   },
   "source": [
    "### Barebones TensorFlow：双层网络\n",
    "我们现在将使用TensorFlow实现我们的第一个神经网络：一个完全连接的ReLU网络，其中有两个隐藏层，CIFAR10数据集上没有偏差。目前我们只使用低级TensorFlow运算符来定义网络;稍后我们将看到如何使用`tf.keras`提供的更高级抽象来简化过程。\n",
    "\n",
    "我们将在函数`two _layer_ fc`中定义网络的正向传递;这将接受TensorFlow Tensors用于网络的输入和权重，并返回TensorFlow Tensor的分数。重要的是要记住，调用`two _layer_ fc`函数**不会**执行任何计算;相反，它只是为正向计算设置计算图。要实际运行网络，我们需要输入TensorFlow会话并将数据提供给计算图。\n",
    "\n",
    "在`two _layer_ fc`函数中定义网络体系结构后，我们将通过设置和运行计算图来测试实现，将零提供给网络并检查输出的形状。\n",
    "\n",
    "阅读并理解此实现非常重要。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_layer_fc(x, params):\n",
    "    \"\"\"\n",
    "    A fully-connected neural network; the architecture is:\n",
    "    fully-connected layer -> ReLU -> fully connected layer.\n",
    "    Note that we only need to define the forward pass here; TensorFlow will take\n",
    "    care of computing the gradients for us.\n",
    "    \n",
    "    The input to the network will be a minibatch of data, of shape\n",
    "    (N, d1, ..., dM) where d1 * ... * dM = D. The hidden layer will have H units,\n",
    "    and the output layer will produce scores for C classes.\n",
    "\n",
    "    Inputs:\n",
    "    - x: A TensorFlow Tensor of shape (N, d1, ..., dM) giving a minibatch of\n",
    "      input data.\n",
    "    - params: A list [w1, w2] of TensorFlow Tensors giving weights for the\n",
    "      network, where w1 has shape (D, H) and w2 has shape (H, C).\n",
    "    \n",
    "    Returns:\n",
    "    - scores: A TensorFlow Tensor of shape (N, C) giving classification scores\n",
    "      for the input data x.\n",
    "    \"\"\"\n",
    "    w1, w2 = params  # Unpack the parameters\n",
    "    # 数据塑型\n",
    "    x = flatten(x)   # Flatten the input; now x has shape (N, D)\n",
    "    # 自带具体网络的实现\n",
    "    h = tf.nn.relu(tf.matmul(x, w1)) # Hidden layer: h has shape (N, H)\n",
    "    # 矩阵相乘\n",
    "    scores = tf.matmul(h, w2)        # Compute scores of shape (N, C)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 10)\n"
     ]
    }
   ],
   "source": [
    "def two_layer_fc_test():\n",
    "    # TensorFlow 的默认计算图是全局变量，为了避免后续使用混乱，需要先清理先前构建的图\n",
    "    # TensorFlow's default computational graph is essentially a hidden global\n",
    "    # variable. To avoid adding to this default graph when you rerun this cell,\n",
    "    # we clear the default graph before constructing the graph we care about.\n",
    "    tf.reset_default_graph()\n",
    "    hidden_layer_size = 42\n",
    "\n",
    "    # Scoping our computational graph setup code under a tf.device context\n",
    "    # manager lets us tell TensorFlow where we want these Tensors to be\n",
    "    # placed.\n",
    "    with tf.device(device):\n",
    "        # Set up a placehoder for the input of the network, and constant\n",
    "        # zero Tensors for the network weights. Here we declare w1 and w2\n",
    "        # using tf.zeros instead of tf.placeholder as we've seen before - this\n",
    "        # means that the values of w1 and w2 will be stored in the computational\n",
    "        # graph itself and will persist across multiple runs of the graph; in\n",
    "        # particular this means that we don't have to pass values for w1 and w2\n",
    "        # using a feed_dict when we eventually run the graph.\n",
    "        x = tf.placeholder(tf.float32)\n",
    "        w1 = tf.zeros((32 * 32 * 3, hidden_layer_size))\n",
    "        w2 = tf.zeros((hidden_layer_size, 10))\n",
    "        \n",
    "        # Call our two_layer_fc function to set up the computational\n",
    "        # graph for the forward pass of the network.\n",
    "        scores = two_layer_fc(x, [w1, w2])\n",
    "    \n",
    "    # Use numpy to create some concrete data that we will pass to the\n",
    "    # computational graph for the x placeholder.\n",
    "    x_np = np.zeros((64, 32, 32, 3))\n",
    "    with tf.Session() as sess:\n",
    "        # The calls to tf.zeros above do not actually instantiate the values\n",
    "        # for w1 and w2; the following line tells TensorFlow to instantiate\n",
    "        # the values of all Tensors (like w1 and w2) that live in the graph.\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        # Here we actually run the graph, using the feed_dict to pass the\n",
    "        # value to bind to the placeholder for x; we ask TensorFlow to compute\n",
    "        # the value of the scores Tensor, which it returns as a numpy array.\n",
    "        scores_np = sess.run(scores, feed_dict={x: x_np})\n",
    "        print(scores_np.shape)\n",
    "\n",
    "two_layer_fc_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Barebones TensorFlow: Three-Layer ConvNet\n",
    "Here you will complete the implementation of the function `three_layer_convnet` which will perform the forward pass of a three-layer convolutional network. The network should have the following architecture:\n",
    "\n",
    "1. A convolutional layer (with bias) with `channel_1` filters, each with shape `KW1 x KH1`, and zero-padding of two\n",
    "2. ReLU nonlinearity\n",
    "3. A convolutional layer (with bias) with `channel_2` filters, each with shape `KW2 x KH2`, and zero-padding of one\n",
    "4. ReLU nonlinearity\n",
    "5. Fully-connected layer with bias, producing scores for `C` classes.\n",
    "\n",
    "**HINT**: For convolutions: https://www.tensorflow.org/api_docs/python/tf/nn/conv2d; be careful with padding!\n",
    "\n",
    "**HINT**: For biases: https://www.tensorflow.org/performance/xla/broadcasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def three_layer_convnet(x, params):\n",
    "    \"\"\"\n",
    "    A three-layer convolutional network with the architecture described above.\n",
    "    \n",
    "    Inputs:\n",
    "    - x: A TensorFlow Tensor of shape (N, H, W, 3) giving a minibatch of images\n",
    "    - params: A list of TensorFlow Tensors giving the weights and biases for the\n",
    "      network; should contain the following:\n",
    "      - conv_w1: TensorFlow Tensor of shape (KH1, KW1, 3, channel_1) giving\n",
    "        weights for the first convolutional layer.\n",
    "      - conv_b1: TensorFlow Tensor of shape (channel_1,) giving biases for the\n",
    "        first convolutional layer.\n",
    "      - conv_w2: TensorFlow Tensor of shape (KH2, KW2, channel_1, channel_2)\n",
    "        giving weights for the second convolutional layer\n",
    "      - conv_b2: TensorFlow Tensor of shape (channel_2,) giving biases for the\n",
    "        second convolutional layer.\n",
    "      - fc_w: TensorFlow Tensor giving weights for the fully-connected layer.\n",
    "        Can you figure out what the shape should be?\n",
    "      - fc_b: TensorFlow Tensor giving biases for the fully-connected layer.\n",
    "        Can you figure out what the shape should be?\n",
    "    \"\"\"\n",
    "    conv_w1, conv_b1, conv_w2, conv_b2, fc_w, fc_b = params\n",
    "    scores = None\n",
    "    ############################################################################\n",
    "    # TODO: Implement the forward pass for the three-layer ConvNet.            #\n",
    "    ############################################################################\n",
    "    # 对x补0操作\n",
    "    x_padded = tf.pad(x, [[0, 0], [2, 2], [2, 2], [0, 0]], 'CONSTANT')\n",
    "    # tf.nn.conv2d(\n",
    "    # input,\n",
    "    # filter,\n",
    "    # strides,\n",
    "    # padding,\n",
    "    # use_cudnn_on_gpu=True,\n",
    "    # data_format='NHWC',\n",
    "    # dilations=[1, 1, 1, 1],\n",
    "    # name=None\n",
    "    # )Computes a 2-D convolution given 4-D input and filter tensors.\n",
    "    conv1 = tf.nn.conv2d(x_padded, conv_w1, [1, 1, 1, 1], padding='VALID') + conv_b1\n",
    "    # 卷积后计算激活函数\n",
    "    relu1 = tf.nn.relu(conv1)\n",
    "    # 补0卷积\n",
    "    conv1_padded = tf.pad(relu1, [[0, 0], [1, 1], [1, 1], [0, 0]], 'CONSTANT')\n",
    "    conv2 = tf.nn.conv2d(conv1_padded, conv_w2, [1, 1, 1, 1], padding='VALID') + conv_b2\n",
    "    relu2 = tf.nn.relu(conv2)\n",
    "    # 调整计算得分\n",
    "    conv2_flattened = flatten(relu2)\n",
    "    fc1 = tf.matmul(conv2_flattened, fc_w) + fc_b\n",
    "    scores = fc1\n",
    "    ############################################################################\n",
    "    #                              END OF YOUR CODE                            #\n",
    "    ############################################################################\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defing the forward pass of the three-layer ConvNet above, run the following cell to test your implementation. Like the two-layer network, we use the `three_layer_convnet` function to set up the computational graph, then run the graph on a batch of zeros just to make sure the function doesn't crash, and produces outputs of the correct shape.\n",
    "\n",
    "When you run this function, `scores_np` should have shape `(64, 10)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores_np has shape:  (64, 10)\n"
     ]
    }
   ],
   "source": [
    "def three_layer_convnet_test():\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    with tf.device(device):\n",
    "        x = tf.placeholder(tf.float32)\n",
    "        conv_w1 = tf.zeros((5, 5, 3, 6))\n",
    "        conv_b1 = tf.zeros((6,))\n",
    "        conv_w2 = tf.zeros((3, 3, 6, 9))\n",
    "        conv_b2 = tf.zeros((9,))\n",
    "        fc_w = tf.zeros((32 * 32 * 9, 10))\n",
    "        fc_b = tf.zeros((10,))\n",
    "        params = [conv_w1, conv_b1, conv_w2, conv_b2, fc_w, fc_b]\n",
    "        scores = three_layer_convnet(x, params)\n",
    "\n",
    "    # Inputs to convolutional layers are 4-dimensional arrays with shape\n",
    "    # [batch_size, height, width, channels]\n",
    "    x_np = np.zeros((64, 32, 32, 3))\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        scores_np = sess.run(scores, feed_dict={x: x_np})\n",
    "        print('scores_np has shape: ', scores_np.shape)\n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "    three_layer_convnet_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "### Barebones TensorFlow: Training Step\n",
    "We now define the `training_step` function which sets up the part of the computational graph that performs a single training step. This will take three basic steps:\n",
    "\n",
    "1. Compute the loss\n",
    "2. Compute the gradient of the loss with respect to all network weights\n",
    "3. Make a weight update step using (stochastic) gradient descent.\n",
    "\n",
    "Note that the step of updating the weights is itself an operation in the computational graph - the calls to `tf.assign_sub` in `training_step` return TensorFlow operations that mutate the weights when they are executed. There is an important bit of subtlety here - when we call `sess.run`, TensorFlow does not execute all operations in the computational graph; it only executes the minimal subset of the graph necessary to compute the outputs that we ask TensorFlow to produce. As a result, naively computing the loss would not cause the weight update operations to execute, since the operations needed to compute the loss do not depend on the output of the weight update. To fix this problem, we insert a **control dependency** into the graph, adding a duplicate `loss` node to the graph that does depend on the outputs of the weight update operations; this is the object that we actually return from the `training_step` function. As a result, asking TensorFlow to evaluate the value of the `loss` returned from `training_step` will also implicitly update the weights of the network using that minibatch of data.\n",
    "\n",
    "We need to use a few new TensorFlow functions to do all of this:\n",
    "- For computing the cross-entropy loss we'll use `tf.nn.sparse_softmax_cross_entropy_with_logits`: https://www.tensorflow.org/api_docs/python/tf/nn/sparse_softmax_cross_entropy_with_logits\n",
    "- For averaging the loss across a minibatch of data we'll use `tf.reduce_mean`:\n",
    "https://www.tensorflow.org/api_docs/python/tf/reduce_mean\n",
    "- For computing gradients of the loss with respect to the weights we'll use `tf.gradients`:  https://www.tensorflow.org/api_docs/python/tf/gradients\n",
    "- We'll mutate the weight values stored in a TensorFlow Tensor using `tf.assign_sub`: https://www.tensorflow.org/api_docs/python/tf/assign_sub\n",
    "- We'll add a control dependency to the graph using `tf.control_dependencies`: https://www.tensorflow.org/api_docs/python/tf/control_dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "zh"
   },
   "source": [
    "### Barebones TensorFlow：训练步骤\n",
    "我们现在定义`training _step`函数，该函数设置执行单个训练步骤的计算图的一部分。这将采取三个基本步骤：\n",
    "\n",
    "1.计算损失\n",
    "2.根据所有网络权重计算损失的梯度\n",
    "3.使用（随机）梯度下降进行重量更新步骤。\n",
    "\n",
    "注意，更新权重的步骤本身就是计算图形中的一个操作 - 在`training_ step`中对`tf.assign _sub`的调用返回TensorFlow操作，这些操作在执行时改变权重。这里有一点点微妙之处 - 当我们调用`sess.run`时，TensorFlow不执行计算图中的所有操作;它只执行计算我们要求TensorFlow生成的输出所需的图的最小子集。结果，天真地计算损失将不会导致权重更新操作被执行，因为计算损失所需的操作不依赖于权重更新的输出。为了解决这个问题，我们在图中插入一个**控制依赖**，向图中添加一个重复的`loss`节点，该节点依赖于权重更新操作的输出;这是我们实际从`training_ step`函数返回的对象。因此，要求TensorFlow评估从`training _step`返回的`loss`的值，也将使用该数据的小批量隐式更新网络的权重。\n",
    "\n",
    "我们需要使用一些新的TensorFlow函数来完成所有这些：\n",
    " - 为了计算交叉熵损失，我们将使用`tf.nn.sparse_ softmax _交叉_熵_和_ logits`：https：//www.tensorflow.org/ API _文档/蟒/ TF / NN /稀疏_ SOFTMAX _横_熵_与_ logits\n",
    " - 为了平均数据的小批量丢失，我们将使用`tf.reduce _mean`：\n",
    "https://www.tensorflow.org/api_docs/python/tf/reduce_mean\n",
    " - 为了计算相对于权重的损失梯度，我们将使用`tf.gradients`：https：//www.tensorflow.org/api_docs/python/tf/gradients\n",
    " - 我们将使用`tf.assign _sub`改变存储在TensorFlow Tensor中的权重值：https：//www.tensorflow.org/api_docs/python/tf/assign_sub\n",
    " - 我们将使用`tf.control_ dependencies`向图形添加控件依赖项：https：//www.tensorflow.org/api_docs/python/tf/control_dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(scores, y, params, learning_rate):\n",
    "    \"\"\"\n",
    "    Set up the part of the computational graph which makes a training step.\n",
    "\n",
    "    Inputs:\n",
    "    - scores: TensorFlow Tensor of shape (N, C) giving classification scores for\n",
    "      the model.\n",
    "    - y: TensorFlow Tensor of shape (N,) giving ground-truth labels for scores;\n",
    "      y[i] == c means that c is the correct class for scores[i].\n",
    "    - params: List of TensorFlow Tensors giving the weights of the model\n",
    "    - learning_rate: Python scalar giving the learning rate to use for gradient\n",
    "      descent step.\n",
    "      \n",
    "    Returns:\n",
    "    - loss: A TensorFlow Tensor of shape () (scalar) giving the loss for this\n",
    "      batch of data; evaluating the loss also performs a gradient descent step\n",
    "      on params (see above).\n",
    "    \"\"\"\n",
    "    # First compute the loss; the first line gives losses for each example in\n",
    "    # the minibatch, and the second averages the losses acros the batch\n",
    "    losses = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=scores)\n",
    "    loss = tf.reduce_mean(losses)\n",
    "\n",
    "    # Compute the gradient of the loss with respect to each parameter of the the\n",
    "    # network. This is a very magical function call: TensorFlow internally\n",
    "    # traverses the computational graph starting at loss backward to each element\n",
    "    # of params, and uses backpropagation to figure out how to compute gradients;\n",
    "    # it then adds new operations to the computational graph which compute the\n",
    "    # requested gradients, and returns a list of TensorFlow Tensors that will\n",
    "    # contain the requested gradients when evaluated.\n",
    "    grad_params = tf.gradients(loss, params)\n",
    "    \n",
    "    # Make a gradient descent step on all of the model parameters.\n",
    "    new_weights = []   \n",
    "    for w, grad_w in zip(params, grad_params):\n",
    "        # 对w的更新操作，不过这个的实现与 w -= learning_rate * grad_w 有什么区别？\n",
    "        new_w = tf.assign_sub(w, learning_rate * grad_w)\n",
    "        new_weights.append(new_w)\n",
    "\n",
    "    # Insert a control dependency so that evaluting the loss causes a weight\n",
    "    # update to happen; see the discussion above.\n",
    "    with tf.control_dependencies(new_weights):\n",
    "        return tf.identity(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "### Barebones TensorFlow: Training Loop\n",
    "Now we set up a basic training loop using low-level TensorFlow operations. We will train the model using stochastic gradient descent without momentum. The `training_step` function sets up the part of the computational graph that performs the training step, and the function `train_part2` iterates through the training data, making training steps on each minibatch, and periodically evaluates accuracy on the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "zh"
   },
   "source": [
    "### Barebones TensorFlow：训练循环\n",
    "现在我们使用低级TensorFlow操作设置基本训练循环。我们将使用没有动量的随机梯度下降来训练模型。 `training _step`函数设置执行训练步骤的计算图的一部分，函数`train_ part2`遍历训练数据，在每个小批量上进行训练，并定期评估准确性在验证集上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_part2(model_fn, init_fn, learning_rate):\n",
    "    \"\"\"\n",
    "    Train a model on CIFAR-10.\n",
    "    \n",
    "    Inputs:\n",
    "    - model_fn: A Python function that performs the forward pass of the model\n",
    "      using TensorFlow; it should have the following signature:\n",
    "      scores = model_fn(x, params) where x is a TensorFlow Tensor giving a\n",
    "      minibatch of image data, params is a list of TensorFlow Tensors holding\n",
    "      the model weights, and scores is a TensorFlow Tensor of shape (N, C)\n",
    "      giving scores for all elements of x.\n",
    "    - init_fn: A Python function that initializes the parameters of the model.\n",
    "      It should have the signature params = init_fn() where params is a list\n",
    "      of TensorFlow Tensors holding the (randomly initialized) weights of the\n",
    "      model.\n",
    "    - learning_rate: Python float giving the learning rate to use for SGD.\n",
    "    \"\"\"\n",
    "    # First clear the default graph\n",
    "    tf.reset_default_graph()\n",
    "    is_training = tf.placeholder(tf.bool, name='is_training')\n",
    "    # Set up the computational graph for performing forward and backward passes,\n",
    "    # and weight updates.\n",
    "    with tf.device(device):\n",
    "        # Set up placeholders for the data and labels\n",
    "        x = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "        y = tf.placeholder(tf.int32, [None])\n",
    "        params = init_fn()           # Initialize the model parameters\n",
    "        scores = model_fn(x, params) # Forward pass of the model\n",
    "        loss = training_step(scores, y, params, learning_rate)\n",
    "\n",
    "    # Now we actually run the graph many times using the training data\n",
    "    with tf.Session() as sess:\n",
    "        # Initialize variables that will live in the graph\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for t, (x_np, y_np) in enumerate(train_dset):\n",
    "            # Run the graph on a batch of training data; recall that asking\n",
    "            # TensorFlow to evaluate loss will cause an SGD step to happen.\n",
    "            feed_dict = {x: x_np, y: y_np}\n",
    "            loss_np = sess.run(loss, feed_dict=feed_dict)\n",
    "            \n",
    "            # Periodically print the loss and check accuracy on the val set\n",
    "            if t % print_every == 0:\n",
    "                print('Iteration %d, loss = %.4f' % (t, loss_np))\n",
    "                check_accuracy(sess, val_dset, x, scores, is_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Barebones TensorFlow: Check Accuracy\n",
    "When training the model we will use the following function to check the accuracy of our model on the training or validation sets. Note that this function accepts a TensorFlow Session object as one of its arguments; this is needed since the function must actually run the computational graph many times on the data that it loads from the dataset `dset`.\n",
    "\n",
    "Also note that we reuse the same computational graph both for taking training steps and for evaluating the model; however since the `check_accuracy` function never evalutes the `loss` value in the computational graph, the part of the graph that updates the weights of the graph do not execute on the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy(sess, dset, x, scores, is_training=None):\n",
    "    \"\"\"\n",
    "    Check accuracy on a classification model.\n",
    "    \n",
    "    Inputs:\n",
    "    - sess: A TensorFlow Session that will be used to run the graph\n",
    "    - dset: A Dataset object on which to check accuracy\n",
    "    - x: A TensorFlow placeholder Tensor where input images should be fed\n",
    "    - scores: A TensorFlow Tensor representing the scores output from the\n",
    "      model; this is the Tensor we will ask TensorFlow to evaluate.\n",
    "      \n",
    "    Returns: Nothing, but prints the accuracy of the model\n",
    "    \"\"\"\n",
    "    num_correct, num_samples = 0, 0\n",
    "    for x_batch, y_batch in dset:\n",
    "        feed_dict = {x: x_batch, is_training: 0}\n",
    "        scores_np = sess.run(scores, feed_dict=feed_dict)\n",
    "        y_pred = scores_np.argmax(axis=1)\n",
    "        num_samples += x_batch.shape[0]\n",
    "        num_correct += (y_pred == y_batch).sum()\n",
    "    acc = float(num_correct) / num_samples\n",
    "    print('Got %d / %d correct (%.2f%%)' % (num_correct, num_samples, 100 * acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Barebones TensorFlow: Initialization\n",
    "We'll use the following utility method to initialize the weight matrices for our models using Kaiming's normalization method.\n",
    "\n",
    "[1] He et al, *Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification\n",
    "*, ICCV 2015, https://arxiv.org/abs/1502.01852"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kaiming_normal(shape):\n",
    "    if len(shape) == 2:\n",
    "        fan_in, fan_out = shape[0], shape[1]\n",
    "    elif len(shape) == 4:\n",
    "        fan_in, fan_out = np.prod(shape[:3]), shape[3]\n",
    "        # 权重初始化的 **使用ReLU神经元时的当前最佳推荐**\n",
    "    return tf.random_normal(shape) * np.sqrt(2.0 / fan_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Barebones TensorFlow: Train a Two-Layer Network\n",
    "We are finally ready to use all of the pieces defined above to train a two-layer fully-connected network on CIFAR-10.\n",
    "\n",
    "We just need to define a function to initialize the weights of the model, and call `train_part2`.\n",
    "\n",
    "Defining the weights of the network introduces another important piece of TensorFlow API: `tf.Variable`. A TensorFlow Variable is a Tensor whose value is stored in the graph and persists across runs of the computational graph; however unlike constants defined with `tf.zeros` or `tf.random_normal`, the values of a Variable can be mutated as the graph runs; these mutations will persist across graph runs. Learnable parameters of the network are usually stored in Variables.\n",
    "\n",
    "You don't need to tune any hyperparameters, but you should achieve accuracies above 40% after one epoch of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss = 2.9732\n",
      "Got 146 / 1000 correct (14.60%)\n",
      "Iteration 100, loss = 1.9950\n",
      "Got 369 / 1000 correct (36.90%)\n",
      "Iteration 200, loss = 1.3985\n",
      "Got 408 / 1000 correct (40.80%)\n",
      "Iteration 300, loss = 1.8142\n",
      "Got 368 / 1000 correct (36.80%)\n",
      "Iteration 400, loss = 1.7414\n",
      "Got 428 / 1000 correct (42.80%)\n",
      "Iteration 500, loss = 1.7526\n",
      "Got 419 / 1000 correct (41.90%)\n",
      "Iteration 600, loss = 1.8215\n",
      "Got 432 / 1000 correct (43.20%)\n",
      "Iteration 700, loss = 2.0247\n",
      "Got 446 / 1000 correct (44.60%)\n"
     ]
    }
   ],
   "source": [
    "def two_layer_fc_init():\n",
    "    \"\"\"\n",
    "    Initialize the weights of a two-layer network, for use with the\n",
    "    two_layer_network function defined above.\n",
    "    \n",
    "    Inputs: None\n",
    "    \n",
    "    Returns: A list of:\n",
    "    - w1: TensorFlow Variable giving the weights for the first layer\n",
    "    - w2: TensorFlow Variable giving the weights for the second layer\n",
    "    \"\"\"\n",
    "    hidden_layer_size = 4000\n",
    "    w1 = tf.Variable(kaiming_normal((3 * 32 * 32, 4000)))\n",
    "    w2 = tf.Variable(kaiming_normal((4000, 10)))\n",
    "    return [w1, w2]\n",
    "\n",
    "learning_rate = 1e-2\n",
    "train_part2(two_layer_fc, two_layer_fc_init, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Barebones TensorFlow: Train a three-layer ConvNet\n",
    "We will now use TensorFlow to train a three-layer ConvNet on CIFAR-10.\n",
    "\n",
    "You need to implement the `three_layer_convnet_init` function. Recall that the architecture of the network is:\n",
    "\n",
    "1. Convolutional layer (with bias) with 32 5x5 filters, with zero-padding 2\n",
    "2. ReLU\n",
    "3. Convolutional layer (with bias) with 16 3x3 filters, with zero-padding 1\n",
    "4. ReLU\n",
    "5. Fully-connected layer (with bias) to compute scores for 10 classes\n",
    "\n",
    "You don't need to do any hyperparameter tuning, but you should see accuracies above 43% after one epoch of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss = 2.8176\n",
      "Got 127 / 1000 correct (12.70%)\n",
      "Iteration 100, loss = 1.9059\n",
      "Got 375 / 1000 correct (37.50%)\n",
      "Iteration 200, loss = 1.6447\n",
      "Got 412 / 1000 correct (41.20%)\n",
      "Iteration 300, loss = 1.7198\n",
      "Got 399 / 1000 correct (39.90%)\n",
      "Iteration 400, loss = 1.5487\n",
      "Got 450 / 1000 correct (45.00%)\n",
      "Iteration 500, loss = 1.6484\n",
      "Got 461 / 1000 correct (46.10%)\n",
      "Iteration 600, loss = 1.6389\n",
      "Got 482 / 1000 correct (48.20%)\n",
      "Iteration 700, loss = 1.6446\n",
      "Got 501 / 1000 correct (50.10%)\n"
     ]
    }
   ],
   "source": [
    "def three_layer_convnet_init():\n",
    "    \"\"\"\n",
    "    Initialize the weights of a Three-Layer ConvNet, for use with the\n",
    "    three_layer_convnet function defined above.\n",
    "    \n",
    "    Inputs: None\n",
    "    \n",
    "    Returns a list containing:\n",
    "    - conv_w1: TensorFlow Variable giving weights for the first conv layer\n",
    "    - conv_b1: TensorFlow Variable giving biases for the first conv layer\n",
    "    - conv_w2: TensorFlow Variable giving weights for the second conv layer\n",
    "    - conv_b2: TensorFlow Variable giving biases for the second conv layer\n",
    "    - fc_w: TensorFlow Variable giving weights for the fully-connected layer\n",
    "    - fc_b: TensorFlow Variable giving biases for the fully-connected layer\n",
    "    \"\"\"\n",
    "    params = None\n",
    "    ############################################################################\n",
    "    # TODO: Initialize the parameters of the three-layer network.              #\n",
    "    ############################################################################\n",
    "    conv_w1 = tf.Variable(kaiming_normal((5, 5, 3, 32)))\n",
    "    conv_b1 = tf.Variable(tf.zeros([32]))\n",
    "    conv_w2 = tf.Variable(kaiming_normal((3, 3, 32, 16)))\n",
    "    conv_b2 = tf.Variable(tf.zeros([16]))\n",
    "    fc_w = tf.Variable(kaiming_normal((32 * 32 * 16, 10)))\n",
    "    fc_b = tf.Variable(tf.zeros([10]))\n",
    "    params = [conv_w1, conv_b1, conv_w2, conv_b2, fc_w, fc_b]\n",
    "    ############################################################\n",
    "    ############################################################################\n",
    "    #                             END OF YOUR CODE                             #\n",
    "    ############################################################################\n",
    "    return params\n",
    "\n",
    "learning_rate = 3e-3\n",
    "train_part2(three_layer_convnet, three_layer_convnet_init, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "# Part III: Keras Model API\n",
    "Implementing a neural network using the low-level TensorFlow API is a good way to understand how TensorFlow works, but it's a little inconvenient - we had to manually keep track of all Tensors holding learnable parameters, and we had to use a control dependency to implement the gradient descent update step. This was fine for a small network, but could quickly become unweildy for a large complex model.\n",
    "\n",
    "Fortunately TensorFlow provides higher-level packages such as `tf.keras` and `tf.layers` which make it easy to build models out of modular, object-oriented layers; `tf.train` allows you to easily train these models using a variety of different optimization algorithms.\n",
    "\n",
    "In this part of the notebook we will define neural network models using the `tf.keras.Model` API. To implement your own model, you need to do the following:\n",
    "\n",
    "1. Define a new class which subclasses `tf.keras.model`. Give your class an intuitive name that describes it, like `TwoLayerFC` or `ThreeLayerConvNet`.\n",
    "2. In the initializer `__init__()` for your new class, define all the layers you need as class attributes. The `tf.layers` package provides many common neural-network layers, like `tf.layers.Dense` for fully-connected layers and `tf.layers.Conv2D` for convolutional layers. Under the hood, these layers will construct `Variable` Tensors for any learnable parameters. **Warning**: Don't forget to call `super().__init__()` as the first line in your initializer!\n",
    "3. Implement the `call()` method for your class; this implements the forward pass of your model, and defines the *connectivity* of your network. Layers defined in `__init__()` implement `__call__()` so they can be used as function objects that transform input Tensors into output Tensors. Don't define any new layers in `call()`; any layers you want to use in the forward pass should be defined in `__init__()`.\n",
    "\n",
    "After you define your `tf.keras.Model` subclass, you can instantiate it and use it like the model functions from Part II.\n",
    "\n",
    "### Module API: Two-Layer Network\n",
    "\n",
    "Here is a concrete example of using the `tf.keras.Model` API to define a two-layer network. There are a few new bits of API to be aware of here:\n",
    "\n",
    "We use an `Initializer` object to set up the initial values of the learnable parameters of the layers; in particular `tf.variance_scaling_initializer` gives behavior similar to the Kaiming initialization method we used in Part II. You can read more about it here: https://www.tensorflow.org/api_docs/python/tf/variance_scaling_initializer\n",
    "\n",
    "We construct `tf.layers.Dense` objects to represent the two fully-connected layers of the model. In addition to multiplying their input by a weight matrix and adding a bias vector, these layer can also apply a nonlinearity for you. For the first layer we specify a ReLU activation function by passing `activation=tf.nn.relu` to the constructor; the second layer does not apply any activation function.\n",
    "\n",
    "Unfortunately the `flatten` function we defined in Part II is not compatible with the `tf.keras.Model` API; fortunately we can use `tf.layers.flatten` to perform the same operation. The issue with our `flatten` function from Part II has to do with static vs dynamic shapes for Tensors, which is beyond the scope of this notebook; you can read more about the distinction [in the documentation](https://www.tensorflow.org/programmers_guide/faq#tensor_shapes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "zh"
   },
   "source": [
    "# 第三部分：Keras Model API\n",
    "使用低级TensorFlow API实现神经网络是了解TensorFlow如何工作的好方法，但它有点不方便 - 我们必须手动跟踪所有持有可学习参数的Tensors，我们必须使用控制依赖来实现梯度下降更新步骤。对于小型网络来说这很好，但对于大型复杂模型来说很快就会变得不合适。\n",
    "\n",
    "幸运的是，TensorFlow提供了更高级的包，例如`tf.keras`和`tf.layers`，这使得用模块化，面向对象的层构建模型变得容易; `tf.train`允许您使用各种不同的优化算法轻松训练这些模型。\n",
    "\n",
    "在笔记本的这一部分，我们将使用`tf.keras.Model` API定义神经网络模型。要实现自己的模型，您需要执行以下操作：\n",
    "\n",
    "1.定义一个子类`tf.keras.model`的新类。为您的班级提供一个描述它的直观名称，“TwoLayerFC”或“ThreeLayerConvNet”。\n",
    "2.在新类的初始化程序`__init__（）`中，将所需的所有层定义为类属性。 `tf.layers`包提供了许多常见的神经网络层，例如用于完全连接层的`tf.layers.Dense`和用于卷积层的`tf.layers.Conv2D`。在引擎盖下，这些层将为任何可学习的参数构建`Variable`Tulsors。 **警告**：不要忘记调用`super（）。__init__（）`作为初始化程序中的第一行！\n",
    "3.为你的类实现`call（）`方法;这实现了模型的正向传递，并定义了网络的*连接*。在`__init__（）中定义的层`实现`__调用__（）`因此它们可以用作将输入张量转换为输出张量的函数对象。不要在`call（）`中定义任何新层;你想要在正向传递中使用的任何层都应该在`__init__（）`中定义。\n",
    "\n",
    "定义`tf.keras.Model`子类后，可以实例化它并像第二部分中的模型函数一样使用它。\n",
    "\n",
    "### 模块API：双层网络\n",
    "\n",
    "下面是使用`tf.keras.Model` API定义双层网络的具体示例。这里有一些新的API需要注意：\n",
    "\n",
    "我们使用`Initializer`对象来设置层的可学习参数的初始值;特别是`tf.variance _scaling_ initializer`给出的行为类似于我们在第二部分中使用的Kaiming初始化方法。你可以在这里阅读更多相关信息：https：//www.tensorflow.org/api_docs/python/tf/variance_scaling_initializer\n",
    "\n",
    "我们构造`tf.layers.Dense`对象来表示模型的两个完全连接的层。除了将它们的输入乘以权重矩阵并添加偏置矢量之外，这些层还可以为您应用非线性。对于第一层，我们通过将`activation = tf.nn.relu`传递给构造函数来指定ReLU激活函数;第二层不应用任何激活功能。\n",
    "\n",
    "不幸的是，我们在第二部分中定义的`flatten`函数与`tf.keras.Model` API不兼容;幸运的是，我们可以使用`tf.layers.flatten`来执行相同的操作。第二部分我们的“flatten”功能的问题与Tensors的静态和动态形状有关，这超出了本笔记本的范围;您可以在[文档]中详细了解区别（https://www.tensorflow.org/programmers_ guide / faq＃tensor_shapes）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 10)\n"
     ]
    }
   ],
   "source": [
    "class TwoLayerFC(tf.keras.Model):\n",
    "    def __init__(self, hidden_size, num_classes):\n",
    "        super().__init__()        \n",
    "        initializer = tf.variance_scaling_initializer(scale=2.0)\n",
    "        self.fc1 = tf.layers.Dense(hidden_size, activation=tf.nn.relu,\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc2 = tf.layers.Dense(num_classes,\n",
    "                                   kernel_initializer=initializer)\n",
    "    def call(self, x, training=None):\n",
    "        x = tf.layers.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def test_TwoLayerFC():\n",
    "    \"\"\" A small unit test to exercise the TwoLayerFC model above. \"\"\"\n",
    "    tf.reset_default_graph()\n",
    "    input_size, hidden_size, num_classes = 50, 42, 10\n",
    "\n",
    "    # As usual in TensorFlow, we first need to define our computational graph.\n",
    "    # To this end we first construct a TwoLayerFC object, then use it to construct\n",
    "    # the scores Tensor.\n",
    "    model = TwoLayerFC(hidden_size, num_classes)\n",
    "    with tf.device(device):\n",
    "        x = tf.zeros((64, input_size))\n",
    "        scores = model(x)\n",
    "\n",
    "    # Now that our computational graph has been defined we can run the graph\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        scores_np = sess.run(scores)\n",
    "        print(scores_np.shape)\n",
    "        \n",
    "test_TwoLayerFC()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Funtional API: Two-Layer Network\n",
    "The `tf.layers` package provides two different higher-level APIs for defining neural network models. In the example above we used the **object-oriented API**, where each layer of the neural network is represented as a Python object (like `tf.layers.Dense`). Here we showcase the **functional API**, where each layer is a Python function (like `tf.layers.dense`) which inputs and outputs TensorFlow Tensors, and which internally sets up Tensors in the computational graph to hold any learnable weights.\n",
    "\n",
    "To construct a network, one needs to pass the input tensor to the first layer, and construct the subsequent layers sequentially. Here's an example of how to construct the same two-layer nework with the functional API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 10)\n"
     ]
    }
   ],
   "source": [
    "def two_layer_fc_functional(inputs, hidden_size, num_classes):     \n",
    "    initializer = tf.variance_scaling_initializer(scale=2.0)\n",
    "    flattened_inputs = tf.layers.flatten(inputs)\n",
    "    fc1_output = tf.layers.dense(flattened_inputs, hidden_size, activation=tf.nn.relu,\n",
    "                                 kernel_initializer=initializer)\n",
    "    scores = tf.layers.dense(fc1_output, num_classes,\n",
    "                             kernel_initializer=initializer)\n",
    "    return scores\n",
    "\n",
    "def test_two_layer_fc_functional():\n",
    "    \"\"\" A small unit test to exercise the TwoLayerFC model above. \"\"\"\n",
    "    tf.reset_default_graph()\n",
    "    input_size, hidden_size, num_classes = 50, 42, 10\n",
    "\n",
    "    # As usual in TensorFlow, we first need to define our computational graph.\n",
    "    # To this end we first construct a two layer network graph by calling the\n",
    "    # two_layer_network() function. This function constructs the computation\n",
    "    # graph and outputs the score tensor.\n",
    "    with tf.device(device):\n",
    "        x = tf.zeros((64, input_size))\n",
    "        scores = two_layer_fc_functional(x, hidden_size, num_classes)\n",
    "\n",
    "    # Now that our computational graph has been defined we can run the graph\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        scores_np = sess.run(scores)\n",
    "        print(scores_np.shape)\n",
    "        \n",
    "test_two_layer_fc_functional()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Model API: Three-Layer ConvNet\n",
    "Now it's your turn to implement a three-layer ConvNet using the `tf.keras.Model` API. Your model should have the same architecture used in Part II:\n",
    "\n",
    "1. Convolutional layer with 5 x 5 kernels, with zero-padding of 2\n",
    "2. ReLU nonlinearity\n",
    "3. Convolutional layer with 3 x 3 kernels, with zero-padding of 1\n",
    "4. ReLU nonlinearity\n",
    "5. Fully-connected layer to give class scores\n",
    "\n",
    "You should initialize the weights of your network using the same initialization method as was used in the two-layer network above.\n",
    "\n",
    "**Hint**: Refer to the documentation for `tf.layers.Conv2D` and `tf.layers.Dense`:\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/layers/Conv2D\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/layers/Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThreeLayerConvNet(tf.keras.Model):\n",
    "    def __init__(self, channel_1, channel_2, num_classes):\n",
    "        super().__init__()\n",
    "        ########################################################################\n",
    "        # TODO: Implement the __init__ method for a three-layer ConvNet. You   #\n",
    "        # should instantiate layer objects to be used in the forward pass.     #\n",
    "        ########################################################################\n",
    "        initializer = tf.variance_scaling_initializer(scale=2.0)\n",
    "        self.conv1=tf.layers.Conv2D(filters=channel_1,kernel_size=(5,5),\n",
    "                                    padding='same',activation=tf.nn.relu,\n",
    "                                    kernel_initializer=initializer,use_bias=True)\n",
    "        self.conv2=tf.layers.Conv2D(filters=channel_2,kernel_size=(3,3),\n",
    "                                    padding='same',activation=tf.nn.relu,\n",
    "                                    kernel_initializer=initializer,use_bias=True)\n",
    "        self.fc = tf.layers.Dense(num_classes,\n",
    "                                   kernel_initializer=initializer)\n",
    "        ########################################################################\n",
    "        #                           END OF YOUR CODE                           #\n",
    "        ########################################################################\n",
    "        \n",
    "    def call(self, x, training=None):\n",
    "        scores = None\n",
    "        ########################################################################\n",
    "        # TODO: Implement the forward pass for a three-layer ConvNet. You      #\n",
    "        # should use the layer objects defined in the __init__ method.         #\n",
    "        ########################################################################\n",
    "        conv1=self.conv1(x)\n",
    "        conv2=self.conv2(x)\n",
    "        sz=conv2.get_shape()\n",
    "        sz=sz[1]*sz[2]*sz[3]\n",
    "        conv2=tf.reshape(conv2,shape=(-1,sz))\n",
    "        scores=self.fc(conv2)\n",
    "        ########################################################################\n",
    "        #                           END OF YOUR CODE                           #\n",
    "        ########################################################################        \n",
    "        return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you complete the implementation of the `ThreeLayerConvNet` above you can run the following to ensure that your implementation does not crash and produces outputs of the expected shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 10)\n"
     ]
    }
   ],
   "source": [
    "def test_ThreeLayerConvNet():\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    channel_1, channel_2, num_classes = 12, 8, 10\n",
    "    model = ThreeLayerConvNet(channel_1, channel_2, num_classes)\n",
    "    with tf.device(device):\n",
    "        x = tf.zeros((64, 3, 32, 32))\n",
    "        scores = model(x)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        scores_np = sess.run(scores)\n",
    "        print(scores_np.shape)\n",
    "\n",
    "test_ThreeLayerConvNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "### Keras Model API: Training Loop\n",
    "We need to implement a slightly different training loop when using the `tf.keras.Model` API. Instead of computing gradients and updating the weights of the model manually, we use an `Optimizer` object from the `tf.train` package which takes care of these details for us. You can read more about `Optimizer`s here: https://www.tensorflow.org/api_docs/python/tf/train/Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "zh"
   },
   "source": [
    "### Keras Model API：训练循环\n",
    "使用`tf.keras.Model` API时，我们需要实现稍微不同的训练循环。我们不是手动计算渐变和更新模型的权重，而是使用`tf.train`包中的`Optimizer`对象来处理这些细节。你可以在这里阅读更多关于`Optimizer`s的信息：https：//www.tensorflow.org/api_docs/python/tf/train/Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_part34(model_init_fn, optimizer_init_fn, num_epochs=1):\n",
    "    \"\"\"\n",
    "    Simple training loop for use with models defined using tf.keras. It trains\n",
    "    a model for one epoch on the CIFAR-10 training set and periodically checks\n",
    "    accuracy on the CIFAR-10 validation set.\n",
    "    \n",
    "    Inputs:\n",
    "    - model_init_fn: A function that takes no parameters; when called it\n",
    "      constructs the model we want to train: model = model_init_fn()\n",
    "    - optimizer_init_fn: A function which takes no parameters; when called it\n",
    "      constructs the Optimizer object we will use to optimize the model:\n",
    "      optimizer = optimizer_init_fn()\n",
    "    - num_epochs: The number of epochs to train for\n",
    "    \n",
    "    Returns: Nothing, but prints progress during trainingn\n",
    "    \"\"\"\n",
    "    tf.reset_default_graph()    \n",
    "    with tf.device(device):\n",
    "        # Construct the computational graph we will use to train the model. We\n",
    "        # use the model_init_fn to construct the model, declare placeholders for\n",
    "        # the data and labels\n",
    "        x = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "        y = tf.placeholder(tf.int32, [None])\n",
    "        \n",
    "        # We need a place holder to explicitly specify if the model is in the training\n",
    "        # phase or not. This is because a number of layers behaves differently in\n",
    "        # training and in testing, e.g., dropout and batch normalization.\n",
    "        # We pass this variable to the computation graph through feed_dict as shown below.\n",
    "        is_training = tf.placeholder(tf.bool, name='is_training')\n",
    "        \n",
    "        # Use the model function to build the forward pass.\n",
    "        scores = model_init_fn(x, is_training)\n",
    "\n",
    "        # Compute the loss like we did in Part II\n",
    "        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=scores)\n",
    "        loss = tf.reduce_mean(loss)\n",
    "\n",
    "        # Use the optimizer_fn to construct an Optimizer, then use the optimizer\n",
    "        # to set up the training step. Asking TensorFlow to evaluate the\n",
    "        # train_op returned by optimizer.minimize(loss) will cause us to make a\n",
    "        # single update step using the current minibatch of data.\n",
    "        \n",
    "        # Note that we use tf.control_dependencies to force the model to run\n",
    "        # the tf.GraphKeys.UPDATE_OPS at each training step. tf.GraphKeys.UPDATE_OPS\n",
    "        # holds the operators that update the states of the network.\n",
    "        # For example, the tf.layers.batch_normalization function adds the running mean\n",
    "        # and variance update operators to tf.GraphKeys.UPDATE_OPS.\n",
    "        optimizer = optimizer_init_fn()\n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            train_op = optimizer.minimize(loss)\n",
    "\n",
    "    # Now we can run the computational graph many times to train the model.\n",
    "    # When we call sess.run we ask it to evaluate train_op, which causes the\n",
    "    # model to update.\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        t = 0\n",
    "        for epoch in range(num_epochs):\n",
    "            print('Starting epoch %d' % epoch)\n",
    "            for x_np, y_np in train_dset:\n",
    "                feed_dict = {x: x_np, y: y_np, is_training:1}\n",
    "                loss_np, _ = sess.run([loss, train_op], feed_dict=feed_dict)\n",
    "                if t % print_every == 0:\n",
    "                    print('Iteration %d, loss = %.4f' % (t, loss_np))\n",
    "                    check_accuracy(sess, val_dset, x, scores, is_training=is_training)\n",
    "                    print()\n",
    "                t += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Model API: Train a Two-Layer Network\n",
    "We can now use the tools defined above to train a two-layer network on CIFAR-10. We define the `model_init_fn` and `optimizer_init_fn` that construct the model and optimizer respectively when called. Here we want to train the model using stochastic gradient descent with no momentum, so we construct a `tf.train.GradientDescentOptimizer` function; you can [read about it here](https://www.tensorflow.org/api_docs/python/tf/train/GradientDescentOptimizer).\n",
    "\n",
    "You don't need to tune any hyperparameters here, but you should achieve accuracies above 40% after one epoch of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0\n",
      "Iteration 0, loss = 3.0855\n",
      "Got 120 / 1000 correct (12.00%)\n",
      "\n",
      "Iteration 100, loss = 1.9058\n",
      "Got 387 / 1000 correct (38.70%)\n",
      "\n",
      "Iteration 200, loss = 1.4724\n",
      "Got 394 / 1000 correct (39.40%)\n",
      "\n",
      "Iteration 300, loss = 1.7896\n",
      "Got 383 / 1000 correct (38.30%)\n",
      "\n",
      "Iteration 400, loss = 1.8340\n",
      "Got 422 / 1000 correct (42.20%)\n",
      "\n",
      "Iteration 500, loss = 1.8252\n",
      "Got 425 / 1000 correct (42.50%)\n",
      "\n",
      "Iteration 600, loss = 1.8489\n",
      "Got 427 / 1000 correct (42.70%)\n",
      "\n",
      "Iteration 700, loss = 1.9909\n",
      "Got 445 / 1000 correct (44.50%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hidden_size, num_classes = 4000, 10\n",
    "learning_rate = 1e-2\n",
    "\n",
    "def model_init_fn(inputs, is_training):\n",
    "    return TwoLayerFC(hidden_size, num_classes)(inputs)\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    return tf.train.GradientDescentOptimizer(learning_rate)\n",
    "\n",
    "train_part34(model_init_fn, optimizer_init_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Model API: Train a Two-Layer Network (functional API)\n",
    "Similarly, we train the two-layer network constructed using the functional API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0\n",
      "Iteration 0, loss = 2.9305\n",
      "Got 144 / 1000 correct (14.40%)\n",
      "\n",
      "Iteration 100, loss = 1.9330\n",
      "Got 385 / 1000 correct (38.50%)\n",
      "\n",
      "Iteration 200, loss = 1.4719\n",
      "Got 380 / 1000 correct (38.00%)\n",
      "\n",
      "Iteration 300, loss = 1.7273\n",
      "Got 377 / 1000 correct (37.70%)\n",
      "\n",
      "Iteration 400, loss = 1.7511\n",
      "Got 406 / 1000 correct (40.60%)\n",
      "\n",
      "Iteration 500, loss = 1.7364\n",
      "Got 430 / 1000 correct (43.00%)\n",
      "\n",
      "Iteration 600, loss = 1.8374\n",
      "Got 443 / 1000 correct (44.30%)\n",
      "\n",
      "Iteration 700, loss = 2.0104\n",
      "Got 446 / 1000 correct (44.60%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hidden_size, num_classes = 4000, 10\n",
    "learning_rate = 1e-2\n",
    "\n",
    "def model_init_fn(inputs, is_training):\n",
    "    return two_layer_fc_functional(inputs, hidden_size, num_classes)\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    return tf.train.GradientDescentOptimizer(learning_rate)\n",
    "\n",
    "train_part34(model_init_fn, optimizer_init_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Model API: Train a Three-Layer ConvNet\n",
    "Here you should use the tools we've defined above to train a three-layer ConvNet on CIFAR-10. Your ConvNet should use 32 filters in the first convolutional layer and 16 filters in the second layer.\n",
    "\n",
    "To train the model you should use gradient descent with Nesterov momentum 0.9. \n",
    "\n",
    "**HINT**: https://www.tensorflow.org/api_docs/python/tf/train/MomentumOptimizer\n",
    "\n",
    "You don't need to perform any hyperparameter tuning, but you should achieve accuracies above 45% after training for one epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0\n",
      "Iteration 0, loss = 2.9159\n",
      "Got 80 / 1000 correct (8.00%)\n",
      "\n",
      "Iteration 100, loss = 1.6883\n",
      "Got 454 / 1000 correct (45.40%)\n",
      "\n",
      "Iteration 200, loss = 1.2591\n",
      "Got 486 / 1000 correct (48.60%)\n",
      "\n",
      "Iteration 300, loss = 1.3364\n",
      "Got 495 / 1000 correct (49.50%)\n",
      "\n",
      "Iteration 400, loss = 1.3541\n",
      "Got 537 / 1000 correct (53.70%)\n",
      "\n",
      "Iteration 500, loss = 1.5426\n",
      "Got 527 / 1000 correct (52.70%)\n",
      "\n",
      "Iteration 600, loss = 1.5414\n",
      "Got 544 / 1000 correct (54.40%)\n",
      "\n",
      "Iteration 700, loss = 1.3174\n",
      "Got 548 / 1000 correct (54.80%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 3e-3\n",
    "channel_1, channel_2, num_classes = 32, 16, 10\n",
    "\n",
    "def model_init_fn(inputs, is_training):\n",
    "    model = None\n",
    "    ############################################################################\n",
    "    # TODO: Complete the implementation of model_fn.                           #\n",
    "    ############################################################################\n",
    "    model=ThreeLayerConvNet(channel_1, channel_2, num_classes)\n",
    "    ############################################################################\n",
    "    #                           END OF YOUR CODE                               #\n",
    "    ############################################################################\n",
    "    return model(inputs)\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    optimizer = None\n",
    "    ############################################################################\n",
    "    # TODO: Complete the implementation of model_fn.                           #\n",
    "    ############################################################################\n",
    "    optimizer=tf.train.MomentumOptimizer (learning_rate=learning_rate,\n",
    "                                          momentum=0.9,use_nesterov=True)\n",
    "    ############################################################################\n",
    "    #                           END OF YOUR CODE                               #\n",
    "    ############################################################################\n",
    "    return optimizer\n",
    "\n",
    "train_part34(model_init_fn, optimizer_init_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "# Part IV: Keras Sequential API\n",
    "In Part III we introduced the `tf.keras.Model` API, which allows you to define models with any number of learnable layers and with arbitrary connectivity between layers.\n",
    "\n",
    "However for many models you don't need such flexibility - a lot of models can be expressed as a sequential stack of layers, with the output of each layer fed to the next layer as input. If your model fits this pattern, then there is an even easier way to define your model: using `tf.keras.Sequential`. You don't need to write any custom classes; you simply call the `tf.keras.Sequential` constructor with a list containing a sequence of layer objects.\n",
    "\n",
    "One complication with `tf.keras.Sequential` is that you must define the shape of the input to the model by passing a value to the `input_shape` of the first layer in your model.\n",
    "\n",
    "### Keras Sequential API: Two-Layer Network\n",
    "Here we rewrite the two-layer fully-connected network using `tf.keras.Sequential`, and train it using the training loop defined above.\n",
    "\n",
    "You don't need to perform any hyperparameter tuning here, but you should see accuracies above 40% after training for one epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "zh"
   },
   "source": [
    "# 第四部分：Keras顺序API\n",
    "在第三部分中，我们介绍了`tf.keras.Model` API，它允许您定义具有任意数量的可学习层以及层之间任意连接的模型。\n",
    "\n",
    "然而，对于许多模型，您不需要这样的灵活性 - 许多模型可以表示为连续的层堆栈，每层的输出作为输入馈送到下一层。如果你的模型适合这种模式，那么有一种更简单的方法来定义你的模型：使用`tf.keras.Sequential`。您不需要编写任何自定义类;您只需使用包含一系列图层对象的列表调用`tf.keras.Sequential`构造函数。\n",
    "\n",
    "`tf.keras.Sequential`的一个复杂因素是你必须通过将值传递给模型中第一层的`input_shape`来定义模型输入的形状。\n",
    "\n",
    "### Keras顺序API：双层网络\n",
    "在这里，我们使用`tf.keras.Sequential`重写双层全连接网络，并使用上面定义的训练循环训练它。\n",
    "\n",
    "你不需要在这里执行任何超参数调整，但是在训练一个纪元后你应该看到精度超过40％。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0\n",
      "Iteration 0, loss = 3.3136\n",
      "Got 139 / 1000 correct (13.90%)\n",
      "\n",
      "Iteration 100, loss = 1.8231\n",
      "Got 363 / 1000 correct (36.30%)\n",
      "\n",
      "Iteration 200, loss = 1.3877\n",
      "Got 382 / 1000 correct (38.20%)\n",
      "\n",
      "Iteration 300, loss = 1.8160\n",
      "Got 371 / 1000 correct (37.10%)\n",
      "\n",
      "Iteration 400, loss = 1.7701\n",
      "Got 405 / 1000 correct (40.50%)\n",
      "\n",
      "Iteration 500, loss = 1.7796\n",
      "Got 436 / 1000 correct (43.60%)\n",
      "\n",
      "Iteration 600, loss = 1.8735\n",
      "Got 419 / 1000 correct (41.90%)\n",
      "\n",
      "Iteration 700, loss = 2.0119\n",
      "Got 440 / 1000 correct (44.00%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-2\n",
    "\n",
    "def model_init_fn(inputs, is_training):\n",
    "    input_shape = (32, 32, 3)\n",
    "    hidden_layer_size, num_classes = 4000, 10\n",
    "    initializer = tf.variance_scaling_initializer(scale=2.0)\n",
    "    layers = [\n",
    "        tf.layers.Flatten(input_shape=input_shape),\n",
    "        tf.layers.Dense(hidden_layer_size, activation=tf.nn.relu,\n",
    "                        kernel_initializer=initializer),\n",
    "        tf.layers.Dense(num_classes, kernel_initializer=initializer),\n",
    "    ]\n",
    "    model = tf.keras.Sequential(layers)\n",
    "    return model(inputs)\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    return tf.train.GradientDescentOptimizer(learning_rate)\n",
    "\n",
    "train_part34(model_init_fn, optimizer_init_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Sequential API: Three-Layer ConvNet\n",
    "Here you should use `tf.keras.Sequential` to reimplement the same three-layer ConvNet architecture used in Part II and Part III. As a reminder, your model should have the following architecture:\n",
    "\n",
    "1. Convolutional layer with 16 5x5 kernels, using zero padding of 2\n",
    "2. ReLU nonlinearity\n",
    "3. Convolutional layer with 32 3x3 kernels, using zero padding of 1\n",
    "4. ReLU nonlinearity\n",
    "5. Fully-connected layer giving class scores\n",
    "\n",
    "You should initialize the weights of the model using a `tf.variance_scaling_initializer` as above.\n",
    "\n",
    "You should train the model using Nesterov momentum 0.9.\n",
    "\n",
    "You don't need to perform any hyperparameter search, but you should achieve accuracy above 45% after training for one epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0\n",
      "Iteration 0, loss = 2.9404\n",
      "Got 109 / 1000 correct (10.90%)\n",
      "\n",
      "Iteration 100, loss = 1.8328\n",
      "Got 366 / 1000 correct (36.60%)\n",
      "\n",
      "Iteration 200, loss = 1.5472\n",
      "Got 423 / 1000 correct (42.30%)\n",
      "\n",
      "Iteration 300, loss = 1.6151\n",
      "Got 431 / 1000 correct (43.10%)\n",
      "\n",
      "Iteration 400, loss = 1.4967\n",
      "Got 455 / 1000 correct (45.50%)\n",
      "\n",
      "Iteration 500, loss = 1.6802\n",
      "Got 454 / 1000 correct (45.40%)\n",
      "\n",
      "Iteration 600, loss = 1.5418\n",
      "Got 478 / 1000 correct (47.80%)\n",
      "\n",
      "Iteration 700, loss = 1.5849\n",
      "Got 477 / 1000 correct (47.70%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def model_init_fn(inputs, is_training):\n",
    "    model = None\n",
    "    ############################################################################\n",
    "    # TODO: Construct a three-layer ConvNet using tf.keras.Sequential.         #\n",
    "    ############################################################################\n",
    "    initializer = tf.variance_scaling_initializer(scale=2.0)\n",
    "    layers=[\n",
    "        tf.layers.Conv2D(filters=channel_1,kernel_size=(5,5),\n",
    "                                    padding='same',activation=tf.nn.relu,\n",
    "                                    kernel_initializer=initializer,use_bias=True),\n",
    "        tf.layers.Conv2D(filters=channel_2,kernel_size=(3,3),\n",
    "                                    padding='same',activation=tf.nn.relu,\n",
    "                                    kernel_initializer=initializer,use_bias=True),\n",
    "        tf.layers.Flatten(),\n",
    "        tf.layers.Dense(num_classes,kernel_initializer=initializer)       \n",
    "    ]\n",
    "    model=tf.keras.Sequential(layers)\n",
    "    ############################################################################\n",
    "    #                            END OF YOUR CODE                              #\n",
    "    ############################################################################\n",
    "    return model(inputs)\n",
    "\n",
    "learning_rate = 5e-4\n",
    "def optimizer_init_fn():\n",
    "    optimizer = None\n",
    "    ############################################################################\n",
    "    # TODO: Complete the implementation of model_fn.                           #\n",
    "    ############################################################################\n",
    "    optimizer=tf.train.MomentumOptimizer (learning_rate=learning_rate,\n",
    "                                          momentum=0.9,use_nesterov=True)\n",
    "    ############################################################################\n",
    "    #                           END OF YOUR CODE                               #\n",
    "    ############################################################################\n",
    "    return optimizer\n",
    "\n",
    "train_part34(model_init_fn, optimizer_init_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "# Part V: CIFAR-10 open-ended challenge\n",
    "\n",
    "In this section you can experiment with whatever ConvNet architecture you'd like on CIFAR-10.\n",
    "\n",
    "You should experiment with architectures, hyperparameters, loss functions, regularization, or anything else you can think of to train a model that achieves **at least 70%** accuracy on the **validation** set within 10 epochs. You can use the `check_accuracy` and `train` functions from above, or you can implement your own training loop.\n",
    "\n",
    "Describe what you did at the end of the notebook.\n",
    "\n",
    "### Some things you can try:\n",
    "- **Filter size**: Above we used 5x5 and 3x3; is this optimal?\n",
    "- **Number of filters**: Above we used 16 and 32 filters. Would more or fewer do better?\n",
    "- **Pooling**: We didn't use any pooling above. Would this improve the model?\n",
    "- **Normalization**: Would your model be improved with batch normalization, layer normalization, group normalization, or some other normalization strategy?\n",
    "- **Network architecture**: The ConvNet above has only three layers of trainable parameters. Would a deeper model do better?\n",
    "- **Global average pooling**: Instead of flattening after the final convolutional layer, would global average pooling do better? This strategy is used for example in Google's Inception network and in Residual Networks.\n",
    "- **Regularization**: Would some kind of regularization improve performance? Maybe weight decay or dropout?\n",
    "\n",
    "### WARNING: Batch Normalization / Dropout\n",
    "Batch Normalization and Dropout **WILL NOT WORK CORRECTLY** if you use the `train_part34()` function with the object-oriented `tf.keras.Model` or `tf.keras.Sequential` APIs; if you want to use these layers with this training loop then you **must use the tf.layers functional API**.\n",
    "\n",
    "We wrote `train_part34()` to explicitly demonstrate how TensorFlow works; however there are some subtleties that make it tough to handle the object-oriented batch normalization layer in a simple training loop. In practice both `tf.keras` and `tf` provide higher-level APIs which handle the training loop for you, such as [keras.fit](https://keras.io/models/sequential/) and [tf.Estimator](https://www.tensorflow.org/programmers_guide/estimators), both of which will properly handle batch normalization when using the object-oriented API.\n",
    "\n",
    "### Tips for training\n",
    "For each network architecture that you try, you should tune the learning rate and other hyperparameters. When doing this there are a couple important things to keep in mind:\n",
    "\n",
    "- If the parameters are working well, you should see improvement within a few hundred iterations\n",
    "- Remember the coarse-to-fine approach for hyperparameter tuning: start by testing a large range of hyperparameters for just a few training iterations to find the combinations of parameters that are working at all.\n",
    "- Once you have found some sets of parameters that seem to work, search more finely around these parameters. You may need to train for more epochs.\n",
    "- You should use the validation set for hyperparameter search, and save your test set for evaluating your architecture on the best parameters as selected by the validation set.\n",
    "\n",
    "### Going above and beyond\n",
    "If you are feeling adventurous there are many other features you can implement to try and improve your performance. You are **not required** to implement any of these, but don't miss the fun if you have time!\n",
    "\n",
    "- Alternative optimizers: you can try Adam, Adagrad, RMSprop, etc.\n",
    "- Alternative activation functions such as leaky ReLU, parametric ReLU, ELU, or MaxOut.\n",
    "- Model ensembles\n",
    "- Data augmentation\n",
    "- New Architectures\n",
    "  - [ResNets](https://arxiv.org/abs/1512.03385) where the input from the previous layer is added to the output.\n",
    "  - [DenseNets](https://arxiv.org/abs/1608.06993) where inputs into previous layers are concatenated together.\n",
    "  - [This blog has an in-depth overview](https://chatbotslife.com/resnets-highwaynets-and-densenets-oh-my-9bb15918ee32)\n",
    "  \n",
    "### Have fun and happy training! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "zh"
   },
   "source": [
    "# 第五部分：CIFAR-10开放式挑战\n",
    "\n",
    "在本节中，您可以尝试在CIFAR-10上使用的任何ConvNet架构。\n",
    "\n",
    "您应该尝试使用体系结构，超参数，损失函数，正则化或其他任何您可以想到的方法来训练一个模型，该模型在**验证**上达到**至少70％**的准确度在10个时代内设定。你可以使用上面的`check _accuracy`和`train`函数，或者你可以实现自己的训练循环。\n",
    "\n",
    "描述你在笔记本电脑末端所做的事情。\n",
    "\n",
    "### 你可以尝试一些事情：\n",
    " -  **过滤器尺寸**：上面我们使用5x5和3x3;这是最佳的吗？\n",
    " -  **过滤器数量**：上面我们使用了16和32个过滤器。会更多或更少做得更好吗？\n",
    " -  **池**：我们没有使用上面的任何池。这会改善模型吗？\n",
    " -  **规范化**：您的模型是否会通过批量规范化，层规范化，组规范化或其他一些规范化策略得到改善？\n",
    " -  **网络架构**：上面的ConvNet只有三层可训练参数。更深层次的模型会做得更好吗？\n",
    " -  **全球平均汇集**：在最终卷积层之后，全球平均汇集会更好吗？此策略用于Google的Inception网络和Residual Networks。\n",
    " -  **正规化**：某种正规化会改善绩效吗？也许体重衰减或辍学？\n",
    "\n",
    "### 警告：批量标准化/删除\n",
    "如果你使用`train_ part34（）`函数和面向对象的`tf.keras.Model`或`tf.keras.Sequential` API，那么批量标准化和丢失**将无法正常工作**;如果你想在这个训练循环中使用这些层，那么你**必须使用tf.layers功能API**。\n",
    "\n",
    "我们编写了`train _part34（）`来明确演示TensorFlow的工作原理;然而，有一些细微之处使得在简单的训练循环中处理面向对象的批量标准化层变得困难。在实践中，`tf.keras`和`tf`都提供了更高级的API来处理训练循环，例如[keras.fit]（https://keras.io/models/sequential/）和[tf。 Estimator]（https://www.tensorflow.org/programmers_指南/估算器），两者都可以在使用面向对象的API时正确处理批量规范化。\n",
    "\n",
    "### 培训提示\n",
    "对于您尝试的每个网络体系结构，您应该调整学习速率和其他超参数。在这样做时，需要记住几件重要的事情：\n",
    "\n",
    " - 如果参数运行良好，您应该会在几百次迭代中看到改进\n",
    " - 记住用于超参数调整的从粗到精的方法：首先测试大量的超参数，仅进行一些训练迭代，以找到完全有效的参数组合。\n",
    " - 一旦找到了一些似乎有用的参数集，就可以更好地搜索这些参数。您可能需要训练更多的时代。\n",
    " - 您应该使用验证集进行超参数搜索，并保存测试集以便根据验证集选择的最佳参数评估体系结构。\n",
    "\n",
    "### 超越\n",
    "如果您有冒险精神，可以使用许多其他功能来尝试提高性能。您**不需要**来实施其中任何一项，但如果您有时间，请不要错过乐趣！\n",
    "\n",
    " - 替代优化器：您可以尝试Adam，Adagrad，RMSprop等。\n",
    " - 替代激活功能，例如泄漏的ReLU，参数ReLU，ELU或MaxOut。\n",
    " - 模特合奏\n",
    " - 数据增加\n",
    " - 新架构\n",
    "   -  [ResNets]（https://arxiv.org/abs/1512.03385），其中前一层的输入被添加到输出中。\n",
    "   -  [DenseNets]（https://arxiv.org/abs/1608.06993），其中对先前层的输入连接在一起。\n",
    "   -  [此博客有深入的概述]（https://chatbotslife.com/resnets-highwaynets-and-densenets-oh-my-9bb15918ee32）\n",
    "  \n",
    "### 玩得开心，快乐！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0\n",
      "Iteration 0, loss = 2.3092\n",
      "Got 124 / 1000 correct (12.40%)\n",
      "\n",
      "Iteration 700, loss = 1.2905\n",
      "Got 577 / 1000 correct (57.70%)\n",
      "\n",
      "Starting epoch 1\n",
      "Iteration 1400, loss = 1.0868\n",
      "Got 631 / 1000 correct (63.10%)\n",
      "\n",
      "Starting epoch 2\n",
      "Iteration 2100, loss = 0.8594\n",
      "Got 648 / 1000 correct (64.80%)\n",
      "\n",
      "Starting epoch 3\n",
      "Iteration 2800, loss = 0.8996\n",
      "Got 690 / 1000 correct (69.00%)\n",
      "\n",
      "Starting epoch 4\n",
      "Iteration 3500, loss = 0.6900\n",
      "Got 696 / 1000 correct (69.60%)\n",
      "\n",
      "Starting epoch 5\n",
      "Iteration 4200, loss = 0.5300\n",
      "Got 709 / 1000 correct (70.90%)\n",
      "\n",
      "Starting epoch 6\n",
      "Iteration 4900, loss = 0.4983\n",
      "Got 711 / 1000 correct (71.10%)\n",
      "\n",
      "Starting epoch 7\n",
      "Iteration 5600, loss = 0.5196\n",
      "Got 714 / 1000 correct (71.40%)\n",
      "\n",
      "Starting epoch 8\n",
      "Iteration 6300, loss = 0.6524\n",
      "Got 717 / 1000 correct (71.70%)\n",
      "\n",
      "Starting epoch 9\n",
      "Iteration 7000, loss = 0.5673\n",
      "Got 694 / 1000 correct (69.40%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def model_init_fn(inputs, is_training):\n",
    "    model = None\n",
    "    ############################################################################\n",
    "    # TODO: Construct a model that performs well on CIFAR-10                   #\n",
    "    ############################################################################\n",
    "    def fire_module(inputs,s1,e1,e3):\n",
    "        fire_squeeze=tf.layers.conv2d(inputs,s1,kernel_size=(1,1),padding='SAME',\n",
    "                                      activation=tf.nn.relu,strides=(1,1),                                      \n",
    "                                      kernel_initializer=tf.glorot_uniform_initializer())\n",
    "        fire_expand1=tf.layers.conv2d(fire_squeeze,e1,kernel_size=(1,1),padding='SAME',\n",
    "                                      activation=tf.nn.relu,strides=(1,1),\n",
    "                                      kernel_initializer=tf.glorot_uniform_initializer())\n",
    "        fire_expand2=tf.layers.conv2d(fire_squeeze,e3,kernel_size=(2,2),padding='SAME',\n",
    "                                     activation=tf.nn.relu,strides=(1,1),\n",
    "                                     kernel_initializer=tf.glorot_uniform_initializer())\n",
    "        fire_merge=tf.concat([fire_expand1,fire_expand2],3)\n",
    "        return fire_merge\n",
    "    conv1=tf.layers.conv2d(inputs,96,kernel_size=(7,7),activation=tf.nn.relu,\n",
    "                           kernel_initializer=tf.glorot_uniform_initializer(),\n",
    "                           strides=(1,1),padding='SAME')\n",
    "    pool1=tf.layers.max_pooling2d(conv1,pool_size=(2,2),strides=(2,2))\n",
    "    fire2=fire_module(pool1,16,64,64)\n",
    "    #fire3=fire_module(fire2,16,64,64)\n",
    "    fire4=fire_module(fire2,32,128,128)\n",
    "    dp4=tf.layers.dropout(inputs=fire4,training=is_training)\n",
    "    pool4=tf.layers.max_pooling2d(dp4,pool_size=(2,2),strides=(2,2))\n",
    "    flat5=tf.layers.flatten(pool4)\n",
    "    net=tf.layers.dense(flat5,units=10,\n",
    "                        kernel_initializer=tf.glorot_uniform_initializer())\n",
    "    ############################################################################\n",
    "    #                            END OF YOUR CODE                              #\n",
    "    ############################################################################\n",
    "    return net\n",
    "\n",
    "pass\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    optimizer = None\n",
    "    ############################################################################\n",
    "    # TODO: Construct an optimizer that performs well on CIFAR-10              #\n",
    "    ############################################################################\n",
    "    optimizer=tf.train.AdamOptimizer(learning_rate=4e-4)\n",
    "    ############################################################################\n",
    "    #                            END OF YOUR CODE                              #\n",
    "    ############################################################################\n",
    "    return optimizer\n",
    "\n",
    "device = '/gpu:0'\n",
    "print_every = 700\n",
    "num_epochs = 10\n",
    "train_part34(model_init_fn, optimizer_init_fn, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Describe what you did \n",
    "\n",
    "In the cell below you should write an explanation of what you did, any additional features that you implemented, and/or any graphs that you made in the process of training and evaluating your network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: \n",
    "A simplified implementation of SqueezeNet. Overfitting can be observed, thus dropout is important (train_part34 does not offer an interface on regularizer). AdamOptimizer is better than others."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "nbTranslate": {
   "displayLangs": [
    "en",
    "zh"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "zh",
   "useGoogleTranslate": true
  },
  "notify_time": "5",
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
